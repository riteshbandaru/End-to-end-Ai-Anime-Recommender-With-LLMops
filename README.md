# ğŸŒ AI Anime Recommender with LLMOps ğŸš€

This project is an **AI-powered Anime Recommendation System** built using **LangChain**, **LLMs**, and **Vector Databases** â€” deployed end-to-end using **Docker**, **Kubernetes**, and **Minikube** on a **GCP Virtual Machine (VM)**.

---

## ğŸ“ Project Structure

Ai_Anime_Recommender/
â”‚
â”œâ”€â”€ app/ # Streamlit frontend
â”œâ”€â”€ chroma_db/ # Vector DB files (Chroma)
â”œâ”€â”€ config/ # Config files
â”œâ”€â”€ data/ # Raw & processed data
â”œâ”€â”€ pipeline/ # Data pipeline for prediction & building
â”œâ”€â”€ src/ # Core logic (recommender, vector store, prompt template, etc.)
â”œâ”€â”€ utils/ # Helper utilities
â”œâ”€â”€ Dockerfile # Docker container config
â”œâ”€â”€ llmops-k8s.yaml # Kubernetes deployment/service config
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ setup.py # Python setup for pip install -e .



---

## âš™ï¸ Features

- ğŸ” Semantic search with vector embeddings
- ğŸ§  LLM-powered anime recommendation
- ğŸ³ Docker containerization
- â˜¸ï¸ Kubernetes deployment using Minikube
- â˜ï¸ Hosted on a Google Cloud VM

---

## ğŸ–¥ï¸ Setup on GCP VM (Ubuntu/Linux)

### 1. âœ… Enable Docker

```bash
sudo systemctl enable docker.service
sudo systemctl enable containerd.service
---
## 2. ğŸ” Verify Docker Installation
bash
Copy
Edit
systemctl status docker
docker ps
docker ps -a
You should see the Docker daemon running.

### 3. ğŸ› ï¸ Install Minikube & kubectl

#### a. Install Minikube

Go to [Minikube installation page](https://minikube.sigs.k8s.io/docs/start/ ), choose:

- **OS:** Linux  
- **Architecture:** x86  
- **Download the binary**

Then run the following commands:

```bash
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 
sudo install minikube-linux-amd64 /usr/local/bin/minikube
b. Start Minikube Cluster
bash
Copy
Edit
minikube start
c. Install kubectl
bash
Copy
Edit
sudo snap install kubectl --classic
kubectl version --client
4. ğŸ” Setup GitHub & Push Code (optional)
```bash

git config --global user.email "your-email@gmail.com"
git config --global user.name "your-username"

git add .
git commit -m "deploy"
git push origin main
ğŸ“¦ 5. Docker Build and Kubernetes Deployment
ğŸ³ Point Docker to Minikube:
bash
Copy
Edit
eval $(minikube docker-env)
ğŸ”¨ Build Docker Image:
bash
Copy
Edit
docker build -t llmops-app:latest .
ğŸ”‘ Add Environment Secrets (for APIs):
bash
Copy
Edit
kubectl create secret generic llmops-secrets \
  --from-literal=GROQ_API_KEY="your_key_here" \
  --from-literal=HUGGINGFACEHUB_API_TOKEN="your_token_here"
ğŸš€ Deploy App with Kubernetes
bash
Copy
Edit
kubectl apply -f llmops-k8s.yaml
ğŸ§  Check Kubernetes Status
bash
Copy
Edit
kubectl get pods
kubectl get svc
minikube status
kubectl cluster-info
ğŸŒ Access the App Publicly
Minikube does not expose LoadBalancer services by default. Use a tunnel + port-forward.

Terminal 1 â€“ Create Tunnel:
bash
Copy
Edit
minikube tunnel
Terminal 2 â€“ Forward Port to Host:
bash
Copy
Edit
kubectl port-forward svc/llmops-service 8501:80 --address 0.0.0.0
Now access it at:

bash
Copy
Edit
http://<your-vm-external-ip>:8501
ğŸ§¾ Kubernetes File Explained (llmops-k8s.yaml)
yaml
Copy
Edit
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llmops-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llmops
  template:
    metadata:
      labels:
        app: llmops
    spec:
      containers:
        - name: llmops-container
          image: llmops-app:latest
          ports:
            - containerPort: 8501
          envFrom:
            - secretRef:
                name: llmops-secrets

---
apiVersion: v1
kind: Service
metadata:
  name: llmops-service
spec:
  type: LoadBalancer
  selector:
    app: llmops
  ports:
    - port: 80
      targetPort: 8501
ğŸ¯ Final Output
You will get a running AI Anime Recommendation App powered by LLMs and served through Kubernetes on GCP:

ğŸ“ URL: http://<your-external-vm-ip>:8501

ğŸ“Œ Notes
Keep tunnel & port-forwarding terminals running for access.

You can later shift from Minikube to real Kubernetes clusters (GKE, EKS, AKS).

Ensure your VM firewall rules allow ingress on port 8501.

ğŸ“¬ Contact
For help, reach out to: riteshbandaru27@gmail.com
GitHub: riteshbandaru

